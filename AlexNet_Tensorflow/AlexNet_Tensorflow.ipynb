{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer: I could not train this network on my GPU (GTX 1060). The similar network built with KERAS was easily trained for the same dataset (Present in this same repo). The resource exhaustion is reaching while training and the GPU could not fit the tensor with such large dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1000)\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "import sys\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "ros_path = '/opt/ros/kinetic/lib/python2.7/dist-packages'\n",
    "\n",
    "if ros_path in sys.path:\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 60276736 / 60270631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Succesfully downloaded', '17flowers.tgz', 60270631, 'bytes.')\n",
      "File Extracted\n",
      "Starting to parse images...\n",
      "Parsing Done!\n"
     ]
    }
   ],
   "source": [
    "# Managing the dataset\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "x, y = oxflower17.load_data(one_hot=True)\n",
    "x_path='/tmp/oxford_flower_17_x.npy'\n",
    "y_path='/tmp/oxford_flower_17_y.npy'\n",
    "import numpy as np\n",
    "np.save(x_path, x)\n",
    "np.save(y_path, y)\n",
    "x = np.load(x_path)\n",
    "y = np.load(y_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test_pre, y_train, y_test_pre = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "x_test, x_validation, y_test, y_validation = train_test_split(x_test_pre, y_test_pre, test_size=0.1)\n",
    "\n",
    "# x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "# y = tf.placeholder(tf.float32, [None, 17]) # no of flower speces in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to generate layers\n",
    "def dense(W, x, b):\n",
    "    z = tf.add((tf.matmul(x, W)),b)\n",
    "    a = tf.nn.relu(z)\n",
    "    return a\n",
    "\n",
    "def maxPool2d(x, kernel_size, stride_size):\n",
    "    return(tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1], strides=[1, stride_size, stride_size, 1],padding=\"SAME\"))\n",
    "\n",
    "def conv2D(x, W, b, stride_size):\n",
    "    xW = tf.nn.conv2d(x, W, strides=[1, stride_size, stride_size, 1],padding=\"SAME\")\n",
    "    z = tf.nn.bias_add(xW, b)\n",
    "    a = tf.nn.relu(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the AlexNet Model\n",
    "tf.reset_default_graph()\n",
    "x_init = tf.contrib.layers.xavier_initializer()\n",
    "n_classes = 17\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "y = tf.placeholder(tf.float32, [None, 17]) # no of flower speces in the dataset\n",
    "\n",
    "def AlexNet(img_input):\n",
    "    \n",
    "    \n",
    "    # 1st conv layer: conv, pool, batch_norm\n",
    "    w_c1 = tf.get_variable(\"w_c1\", [11,11,3,96], initializer=x_init)\n",
    "    b_c1 = tf.Variable(tf.zeros([96]))\n",
    "    c1 = conv2D(img_input, w_c1, b_c1, stride_size=4)\n",
    "    p1 = maxPool2d(c1, kernel_size=2, stride_size=2)\n",
    "    bn1 = tf.contrib.layers.batch_norm(p1)\n",
    "    \n",
    "    # 2nd conv layer: conv,pool, batch_norn\n",
    "    w_c2 = tf.get_variable(\"w_c2\", [5,5,96,256], initializer=x_init)\n",
    "    b_c2 = tf.Variable(tf.zeros([256]))\n",
    "    c2 = conv2D(bn1, w_c2, b_c2, stride_size=1)\n",
    "    p2 = maxPool2d(c2, kernel_size=2, stride_size=1)\n",
    "    bn2 = tf.contrib.layers.batch_norm(p2)\n",
    "    \n",
    "    # 3rd conv layer: conv, norm, (no pooling)\n",
    "    w_c3 = tf.get_variable(\"w_c3\", [3,3,256,384], initializer=x_init)\n",
    "    b_c3 = tf.Variable(tf.zeros([384]))\n",
    "    c3 = conv2D(bn2, w_c3, b_c3, stride_size=1)\n",
    "    bn3 = tf.contrib.layers.batch_norm(c3)\n",
    "    \n",
    "    # 4th conv layer: conv, norm, (no pooling)\n",
    "    w_c4 = tf.get_variable(\"w_c4\", [3,3,384,384], initializer=x_init)\n",
    "    b_c4 = tf.Variable(tf.zeros([384]))\n",
    "    c4 = conv2D(bn3, w_c4, b_c4, stride_size=1)\n",
    "    bn4 = tf.contrib.layers.batch_norm(c4)\n",
    "    \n",
    "    # 5th conv layer: conv, pool, norm\n",
    "    w_c5 = tf.get_variable(\"w_c5\", [3,3,384,256], initializer=x_init)\n",
    "    b_c5 = tf.Variable(tf.zeros([256]))\n",
    "    c5 = conv2D(bn4, w_c5, b_c5, stride_size=1)\n",
    "    p3 = maxPool2d(c5, kernel_size=2, stride_size=2)\n",
    "    bn5 = tf.contrib.layers.batch_norm(p3)\n",
    "    \n",
    "    # 1st dense layer: flatten the conv layer\n",
    "    \n",
    "    flattened = tf.reshape(bn5, [-1, 12*12*256])\n",
    "    \n",
    "    w_d1 = tf.get_variable(\"w_d1\", [12*12*256,4096], initializer=x_init)\n",
    "    b_d1 = tf.Variable(tf.zeros([4096]))\n",
    "    d1 = dense(w_d1, flattened, b_d1)\n",
    "    drop_d1 = tf.nn.dropout(d1, 0.5)\n",
    "    \n",
    "    # 2nd dense layer\n",
    "    w_d2 = tf.get_variable(\"w_d2\", [4096,4096], initializer=x_init)\n",
    "    b_d2 = tf.Variable(tf.zeros([4096]))\n",
    "    d2 = dense(w_d2, drop_d1, b_d2)\n",
    "    drop_d2 = tf.nn.dropout(d2, 0.5)\n",
    "    \n",
    "    # 3rd dense layer\n",
    "    w_d3 = tf.get_variable(\"w_d3\", [4096,1000], initializer=x_init)\n",
    "    b_d3 = tf.Variable(tf.zeros([1000]))\n",
    "    d3 = dense(w_d3, drop_d2, b_d3)\n",
    "    drop_d3 = tf.nn.dropout(d3, 0.5)\n",
    "    \n",
    "    # Output layer\n",
    "    w_out = tf.get_variable(\"w_out\", [1000, n_classes], initializer=x_init)\n",
    "    b_out = tf.Variable(tf.zeros([n_classes]))\n",
    "    out = tf.add((tf.matmul(drop_d3, w_out)), b_out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = AlexNet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "initializer_op = tf.global_variables_initializer()\n",
    "\n",
    "# defining model evaluation metrics\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "config_proto = tf.ConfigProto(device_count = {'GPU': 2})\n",
    "sess = tf.Session(config=config_proto)\n",
    "sess.run(initializer_op)\n",
    "\n",
    "epochs = 1    \n",
    "batch_size = 8\n",
    "for epoch in range(epochs):\n",
    "    n_batches = int(x_train.shape[0] / batch_size)\n",
    "    \n",
    "    counter = 1\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        rand_idx = np.random.randint(1088, size=batch_size)\n",
    "        \n",
    "        feed = {\n",
    "            x: x_train[rand_idx],\n",
    "            y: y_train[rand_idx]\n",
    "        }\n",
    "        \n",
    "        sess.run(optimizer, feed_dict=feed)\n",
    "#         _, batch_cost, batch_acc = sess.run([optimizer, cost, accuracy_pct], feed_dict=feed)\n",
    "        \n",
    "#         counter+=1\n",
    "    \n",
    "#     test_cost = cost.eval({x: x_test, y: y_test})\n",
    "#     test_acc_pct = accuracy_pct.eval({x: x_test, y: y_test})\n",
    "#     print(\"Epoch {}: Training Cost = {:.3f}, Training Acc = {:.2f} -- Test Cost = {:.3f}, Test Acc = {:.2f}\"\\\n",
    "#               .format(epoch + 1, avg_cost, avg_acc_pct, test_cost, test_acc_pct))\n",
    "    \n",
    "# test_cost = cost.eval({x: x_test, y: y_test})\n",
    "# test_accy_pct = accuracy_pct.eval({x: x_test, y: y_test})  \n",
    "\n",
    "print(\"Test Cost:\", '{:.3f}'.format(test_cost))\n",
    "print(\"Test Accuracy: \", '{:.2f}'.format(test_accy_pct), \"%\", sep='')\n",
    "print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
