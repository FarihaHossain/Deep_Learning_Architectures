AlexNet has 8 layers: 5 are conv and 3 are fully connected

In between some layers are max pooling and activations(ReLU)

ReLu trains much faster than tanh an in the paper experimental proof is shown which proved ReLu to be approx 6 times faster than the tanh non-linear activation

This architecture also solved the major problem of over-fitting by using Dropout layer after every fully connected layer

Formula for calculating output size: 0 = ((W - k + 2p)/ s ) + 1 , where W is input height; k is kernel size; p is padding; s is stride


